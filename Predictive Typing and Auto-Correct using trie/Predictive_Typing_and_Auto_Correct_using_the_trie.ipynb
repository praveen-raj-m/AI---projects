{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predictive Typing and Auto-Correct using the trie.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMGSnARM8xbQ"
      },
      "source": [
        "Write a python program to extract the contents (excluding any tags) from two \n",
        "websites\n",
        "https://en.wikipedia.org/wiki/Artificial_intelligence\n",
        "https://en.wikipedia.org/wiki/Machine_learning\n",
        "\n",
        "Save the content in two separate files. Construct a trie based on the content retrieved \n",
        "in using HashMap / B-Tree / Dictionary. Write a program to show the implementation \n",
        "of Predictive Typing and Auto-Correct using the trie prepared."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypQ_5TSe9EYr"
      },
      "source": [
        "# creating trie node structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CzdUnkasTi5"
      },
      "source": [
        "class TrieNode(): \n",
        "\tdef __init__(self): \n",
        "\t\tself.children = {} \n",
        "\t\tself.last = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXr_CH-D9JaG"
      },
      "source": [
        "# creating Trie class which handles words inputting, suggesting and correcting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJc0Mp3Muhfa"
      },
      "source": [
        "import nltk\n",
        "class Trie(): \n",
        "  def __init__(self): \n",
        "    self.root = TrieNode() \n",
        "    self.word_list = [] \n",
        "\n",
        "  def formTrie(self, keys): \n",
        "    for key in keys: \n",
        "      self.insert(key)\n",
        "\n",
        "  def insert(self, key): \n",
        "    node = self.root \n",
        "\n",
        "    for a in list(key): \n",
        "      if not node.children.get(a): \n",
        "        node.children[a] = TrieNode() \n",
        "\n",
        "      node = node.children[a] \n",
        "\n",
        "    node.last = True\n",
        "\n",
        "\n",
        "\n",
        "  def suggestionsRec(self, node, word): \n",
        "    if node.last: \n",
        "      self.word_list.append(word) \n",
        "\n",
        "    for a,n in node.children.items(): \n",
        "      self.suggestionsRec(n, word + a) \n",
        "\n",
        "  def printAutoSuggestions(self, key):\n",
        "    node = self.root \n",
        "    not_found = False\n",
        "    temp_word = '' \n",
        "    self.word_list = [] \n",
        "\n",
        "    for a in list(key): \n",
        "      if not node.children.get(a): \n",
        "        not_found = True\n",
        "        break\n",
        "\n",
        "      temp_word += a \n",
        "      node = node.children[a] \n",
        "\n",
        "    if not_found: \n",
        "      return 0\n",
        "    elif node.last and not node.children: \n",
        "      return -1\n",
        "\n",
        "    self.suggestionsRec(node, temp_word) \n",
        "\n",
        "    for s in self.word_list: \n",
        "      print(s) \n",
        "    return 1\n",
        "\n",
        "  def printAutoCorrect(self, key):\n",
        "    node = self.root \n",
        "    temp_word = ''\n",
        "    self.word_list = [] \n",
        "\n",
        "    for a in list(key): \n",
        "      if not node.children.get(a): \n",
        "        break\n",
        "\n",
        "      temp_word += a \n",
        "      node = node.children[a] \n",
        "\n",
        "    self.suggestionsRec(node, temp_word) \n",
        "\n",
        "    for s in self.word_list:\n",
        "      if nltk.edit_distance(key,s) <=3:\n",
        "        print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBkT6uQm9SH5"
      },
      "source": [
        "# Reading data from given websites and removing tags and stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGwQowbRvAMu"
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib import request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1-ZbKH1vAwu"
      },
      "source": [
        "url1 = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
        "url2 = \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
        "html1 = request.urlopen(url1).read().decode('utf8')\n",
        "html2 = request.urlopen(url2).read().decode('utf8')\n",
        "raw1 = BeautifulSoup(html1, 'html.parser').get_text()\n",
        "raw2 = BeautifulSoup(html2, 'html.parser').get_text()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17OtVmgFvCjl",
        "outputId": "fc0e236e-ff93-42cb-e1a4-7a185099a5f8"
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlKboz5UvEYl"
      },
      "source": [
        "words1 = word_tokenize(raw1)\n",
        "words2 = word_tokenize(raw2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biVlzKwZvGBT"
      },
      "source": [
        "filtered1 = []\n",
        "filtered2 = []\n",
        "nlp = spacy.load(\"en\")\n",
        "\n",
        "additional = ['.',',','\\'','\"','?','{','}','[',']','(',')','<','>','!']\n",
        "for i in additional:\n",
        "  nlp.Defaults.stop_words.add(i)\n",
        "\n",
        "\n",
        "for i in words1:\n",
        "  if nlp.vocab[i].is_stop == False:\n",
        "    filtered1.append(i)\n",
        "\n",
        "for i in words2:\n",
        "  if nlp.vocab[i].is_stop == False:\n",
        "    filtered2.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LvI2wn49bh2"
      },
      "source": [
        "# Saving content into 2 different files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28oGALr3vHxx"
      },
      "source": [
        "with open(\"index1.txt\", \"w\") as output:\n",
        "    for i in filtered1:\n",
        "      output.write(str(i)+ \"\\n\")\n",
        "\n",
        "with open(\"index2.txt\", \"w\") as output:\n",
        "    for i in filtered2:\n",
        "      output.write(str(i)+ \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NsRJj4F9f6_"
      },
      "source": [
        "# Predictive Typing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ6joBcDvzql",
        "outputId": "3aece2ca-beb1-491e-98e9-26f03fd0bf64"
      },
      "source": [
        "key = \"hel\" \n",
        "\n",
        "t1 = Trie() \n",
        "t2 = Trie()\n",
        "\n",
        "t1.formTrie(list(set(filtered1)))\n",
        "t2.formTrie(list(set(filtered2)))\n",
        "\n",
        "print(\"Aritificial intelligence\")\n",
        "comp1 = t1.printAutoSuggestions(key) \n",
        "\n",
        "if comp1 == -1: \n",
        "\tprint(\"No other strings found with this prefix\\n\") \n",
        "elif comp1 == 0: \n",
        "\tprint(\"No string found with this prefix\\n\") \n",
        " \n",
        "\n",
        "print(\"\\nMachine Learning\")\n",
        "comp2 = t2.printAutoSuggestions(key) \n",
        "\n",
        "if comp2 == -1: \n",
        "\tprint(\"No other strings found with this prefix\\n\") \n",
        "elif comp2 == 0: \n",
        "\tprint(\"No string found with this prefix\\n\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Aritificial intelligence\n",
            "help\n",
            "helping\n",
            "helps\n",
            "helpful\n",
            "held\n",
            "\n",
            "Machine Learning\n",
            "held\n",
            "help\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6V2aB-49mZ6"
      },
      "source": [
        "# Auto Correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBV2xl-C2JtK",
        "outputId": "b7832d88-f355-4f91-eb37-68544be5eb7c"
      },
      "source": [
        "key = \"machineee\"\n",
        "print(\"Aritificial intelligence\")\n",
        "t1.printAutoCorrect(key)  \n",
        "\n",
        "print(\"\\nMachine Learning\")\n",
        "t2.printAutoCorrect(key)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Aritificial intelligence\n",
            "machine\n",
            "machines\n",
            "machinery\n",
            "\n",
            "Machine Learning\n",
            "machine\n",
            "machines\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}